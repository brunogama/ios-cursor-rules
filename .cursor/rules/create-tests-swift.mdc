---
description: 
globs: 
alwaysApply: false
---
# Swift Testing Best Practices Development Workflow System

Rule for managing the complete development lifecycle including task tracking, testing, and version control.

<rule>
name: development_workflow_system
filters:
  # Task management filters
  - type: event
    pattern: "task_start"
  - type: event
    pattern: "task_complete"
  - type: command
    pattern: "task"
  - type: event
    pattern: "user_request"
  - type: event 
    pattern: "implementation_complete"
  
  # Testing filters
  - type: event
    pattern: "implementation_start"
  - type: command
    pattern: "test"
  - type: file_change
    pattern: "src/*"
  - type: file_change
    pattern: "tests/*"
  - type: event
    pattern: "test_failure"
  
  # Git commit filters
  - type: event
    pattern: "build_success"
  - type: event
    pattern: "test_success"
  - type: event
    pattern: "file_save"
  - type: file_change
    pattern: "*"
  
  # README management filters
  - type: command
    pattern: "readme"
  - type: file_change
    pattern: "README.md"

actions:
  #
  # SECTION 1: TASK MANAGEMENT
  #
  
  - For each function created during the development cycle one test should be implemented together to assert the code generated AND IMPLEMENTED PASSING.
     - For each specification, create appropriate test files:
        - Create test files in the proper location and format
        - Link tests to the specification requirements
        - Add placeholder tests for each requirement
        - Include references to the specs and task
This ensures tests are created before implementation, supporting 
test-driven development.
  
  #
  # SECTION 2: TESTING MANAGEMENT
  #

      
      2. Process test results:
         - If tests pass, update active task to reflect passing tests
         - If tests fail, create detailed failure report
         - Trigger appropriate success/failure events
      
      3. Update task acceptance criteria:
         - Mark "Unit tests pass" criteria based on results
      
      4. If tests pass, check if README needs updating:
         - Analyze significant code changes
         - Compare with current README content
         - Suggest README updates if needed
      
      This validates implementation quality through automated testing and keeps
      documentation in sync with implementation.

  - type: react
    event: "test_failure"
    action: |
      # Analyze test failures and suggest fixes
      
      When tests fail, I'll:
      
      1. Analyze the failure details:
         - Identify failing tests
         - Determine failure reasons
         - Isolate problematic code
      
      2. Create a learning about the test failure:
         - Document the failure details
         - Record potential solutions
         - Link to relevant code and specifications
      
      3. Store the learning for future reference:
         - Save to `.cursor/learnings/` with proper ID (managed by the knowledge_management rule)
         - Update the learnings index
      
      This captures valuable information from failures and helps prevent 
      similar issues in the future.

  - type: react
    event: "file_change"
    conditions:
      - pattern: "src/.*\\.(js|ts|jsx|tsx|py|rs|go|java|rb|cpp|c|h|hpp)$"
    action: |
      # Check for test files when source files change
      
      This ensures all components have corresponding test coverage and 
      tracks changes that might require documentation updates.
  
  #
  # SECTION 3: VERSION CONTROL
  #
  
  - type: react
    conditions:
      - pattern: "file_change|file_save"
    action: |
      # Automatically commit changes using conventional commits format
      
      When a file is changed or saved, I'll:
      1. Determine the appropriate commit type based on the change:
         - `feat`: For new features or functionality
         - `fix`: For bug fixes
         - `docs`: For documentation changes (including specs)
         - `style`: For formatting changes that don't affect code
         - `refactor`: For code restructuring without feature changes
         - `perf`: For performance improvements
         - `test`: For adding or correcting tests
         - `chore`: For maintenance tasks and build changes

      2. Extract scope from the file path (directory structure)
      
      3. Create a commit message in the format: `type(scope): description`
      
      4. For spec files, I'll use the format: `docs(specs): update specifications for <component>`
  
  #
  # SECTION 4: README MANAGEMENT
  #
  
  - type: react
    conditions:
      - pattern: "readme check|check readme"
    action: |
      # Check if README needs updating
      
      I'll analyze the README file and codebase to identify documentation gaps:
      
      1. Compare README content with actual project features:
         - Look for implemented features not described in README
         - Check if API documentation matches current implementation
         - Verify installation instructions are correct
         - Ensure usage examples reflect current behavior
      
      2. Generate a README validation report:
         - List missing or outdated sections
         - Provide specific update recommendations
         - Prioritize documentation gaps by importance
      
      This helps keep documentation in sync with implementation.

  - type: react
    conditions:
      - pattern: "readme update|update readme"
    action: |
      # Update README to reflect current project state
      
      I'll update the README file to match the current state of the project:
      
      1. Preserve existing structure while adding/updating content:
         - Add missing feature descriptions
         - Update outdated API documentation
         - Refresh installation instructions if needed
         - Update usage examples for changed functionality
      
      2. Generate a commit for the README changes:
         - Use commit type `docs`
         - Include scope `readme`
         - Provide descriptive message about updates
      
      This ensures documentation accurately reflects the current implementation.
  
  #
  # SECTION 5: WORKFLOW INTEGRATION
  #
  
  - type: react
    event: "implementation_complete"
    action: |
      # Integrated workflow when implementation is complete
      
      When implementation is complete, I'll:
      
      1. Run verification tests:
         - Execute tests based on project type
         - Verify all tests pass
      
      2. If tests pass:
         - Update the task status to Done (✅)
         - Set end date to current date
         - Extract and save any learnings (via knowledge_management rule)
         - Mark associated specs as completed (via specification_management rule)
         - Update the task index
         - Check if README needs updating based on implemented features
         - Commit the changes with appropriate message
      
      3. If tests fail:
         - Record verification failure
         - Request fixes before allowing completion
         - Do not commit the changes
      
      This ensures only properly tested implementations are marked complete
      and committed to version control with up-to-date documentation.

  - type: suggest
    message: |
      ### Development Workflow System

      Your development process is managed through an integrated workflow:

      **Task Management:**
      - `task create` - Create a new task
      - `task start` - Mark a task as active
      - `task done` - Mark a task as complete
      - `task list` - Show all tasks with status
      - Automatic task creation for implementation requests

      **Testing Framework:**
      - `test run` - Execute tests for the project
      - Automatic test file creation when implementation starts
      - Test coverage verification for source files
      - Test failure analysis and learning capture

      **Version Control:**
      - Automatic commits using conventional format
      - Commit prevention for failing tests
      - Appropriate commit types based on change type
      - Proper scoping based on file structure

      **README Management:**
      - `readme check` - Validate README against current project state
      - `readme update` - Update README to reflect implemented features
      - Automatic README validation after successful tests
      - Documentation kept in sync with implementation

      **Integrated Behaviors:**
      - Implementation request → Create specs → Create task → Create tests → Implement
      - Implementation complete → Run tests → Update task → Check README → Capture learnings → Commit changes
      - Test failure → Analyze issues → Create learnings → Prevent completion

      This system ensures a consistent, high-quality development process from
      task creation through testing to version control, with documentation
      that stays in sync with your codebase.

examples:
  # Task Management Examples
  - input: |
      task create "Implement user authentication flow"
    output: "Task created: TASK-2025-03-05-01 - Implement user authentication flow"

  - input: |
      task list
    output: "Listing all tasks with their current status..."

  # Testing Examples
  - input: |
      test run
    output: "Running tests... ✅ Tests passed successfully! Task updated to reflect passing tests."

  # Version Control Examples
  - input: |
      # After adding a new function
      feat(auth): add user authentication function
    output: "Changes committed with message: feat(auth): add user authentication function"

  # README Management Examples
  - input: |
      readme check
    output: "README validation complete. Found 2 sections that need updating to reflect recent changes."

  # Integrated Workflow Examples
  - input: |
      Implementation complete for user authentication
    output: "Running verification tests... ✅ Implementation verified by tests. README update suggested for new auth feature. Task marked as complete and changes committed."

metadata:
  priority: high
  version: 1.0
</rule> 

# Swift Testing Best Practices for Test-Driven Development (TDD)

As an AI assistant driving iOS development through testing, I MUST follow these guidelines to create effective, maintainable, and reliable tests, ensuring code quality and resilience. The primary goal is to practice TDD: **write tests *before* production code.**

## General Testing Philosophy

- **TDD First**: Write a failing test *before* writing the implementation code.
- **Test Behavior, Not Implementation**: Focus on *what* the code should do, not *how* it does it.
- **SOLID Alignment**: Design tests to validate contracts defined by protocols and interfaces (Dependency Inversion).
- **Single Responsibility Principle (SRP)**: Each test method should validate one specific aspect or behavior.
- **Object Calisthenics**: Keep test methods small, focused, and avoid excessive setup. Aim for clarity.
- **Keep Tests Simple, Fast, and Independent**: Avoid dependencies between tests.
- **One Assertion Per Test**: Strive for one logical assertion per test method to pinpoint failures accurately (SRP).

## XCTest Framework Basics

### Unit Test Structure

- Name test methods with format `test_<methodName>_<scenario>_<expectedResult>`
- Use XCTAssert functions for assertions (XCTAssertEqual, XCTAssertTrue, etc.)
- Organize tests in a logical, consistent manner
- Use setUp() and tearDown() methods for common initialization and cleanup

```swift
class CartTests: XCTestCase {
    // MARK: - Properties
    private var cart: ShoppingCart!
    private var mockPricingService: MockPricingService!
    
    // MARK: - Setup & Teardown
    override func setUp() {
        super.setUp()
        mockPricingService = MockPricingService()
        cart = ShoppingCart(pricingService: mockPricingService)
    }
    
    override func tearDown() {
        cart = nil
        mockPricingService = nil
        super.tearDown()
    }
    
    // MARK: - Tests
    func test_addItem_withValidProduct_increasesItemCount() {
        // Arrange
        let initialCount = cart.itemCount
        let product = Product(id: "test", name: "Test Product", price: 10.0)
        
        // Act
        cart.add(product: product)
        
        // Assert
        XCTAssertEqual(cart.itemCount, initialCount + 1, "Adding a product should increase item count by 1")
    }
    
    func test_calculateTotal_withTaxRate_returnsSumPlusTax() {
        // Arrange
        let product1 = Product(id: "p1", name: "Product 1", price: 10.0)
        let product2 = Product(id: "p2", name: "Product 2", price: 15.0)
        cart.add(product: product1)
        cart.add(product: product2)
        
        // Act
        let total = cart.calculateTotal(withTaxRate: 0.1)
        
        // Assert
        XCTAssertEqual(total, 27.5, "Total should be sum plus tax: (10 + 15) * 1.1 = 27.5")
    }
}
```

### UI Tests

- Use XCUITest for automated UI testing.
- Tests MUST follow the **Page Object Model (POM)** pattern for maintainability. Structure UI tests by screen or logical flow.
- Test critical user flows, not every minor interaction.

```swift
class LoginUITests: XCTestCase {
    let app = XCUIApplication()
    
    override func setUp() {
        super.setUp()
        continueAfterFailure = false
        app.launch()
    }
    
    func test_login_withValidCredentials_showsHomeScreen() {
        // Enter credentials
        let emailTextField = app.textFields["emailTextField"]
        let passwordTextField = app.secureTextFields["passwordTextField"]
        let loginButton = app.buttons["loginButton"]
        
        emailTextField.tap()
        emailTextField.typeText("test@example.com")
        
        passwordTextField.tap()
        passwordTextField.typeText("password123")
        
        loginButton.tap()
        
        // Verify home screen appeared
        let homeTitle = app.staticTexts["homeTitle"]
        XCTAssertTrue(homeTitle.waitForExistence(timeout: 2.0), "Home screen should appear after successful login")
    }
}
```

## Mocking and Stubbing

### Creating Test Doubles

- Create mocks/stubs for dependencies to isolate the System Under Test (SUT).
- **Use protocols** (Dependency Inversion) to define interfaces, making mocking easier and promoting loose coupling (SOLID).

```swift
// Protocol that enables mocking
protocol NetworkService {
    func fetchData(from url: URL, completion: @escaping (Result<Data, Error>) -> Void)
}

// Mock implementation for testing
class MockNetworkService: NetworkService {
    var dataToReturn: Data?
    var errorToReturn: Error?
    var capturedURL: URL?
    
    func fetchData(from url: URL, completion: @escaping (Result<Data, Error>) -> Void) {
        capturedURL = url
        
        if let error = errorToReturn {
            completion(.failure(error))
        } else if let data = dataToReturn {
            completion(.success(data))
        }
    }
}

// Using the mock in tests
func test_fetchUserProfile_withSuccess_returnsUserObject() {
    // Arrange
    let mockService = MockNetworkService()
    let userJSON = """
    {"id": "123", "name": "Test User", "email": "test@example.com"}
    """.data(using: .utf8)!
    
    mockService.dataToReturn = userJSON
    let userRepository = UserRepository(networkService: mockService)
    
    let expectation = XCTestExpectation(description: "Fetch user profile")
    var fetchedUser: User?
    
    // Act
    userRepository.fetchUser(id: "123") { result in
        if case .success(let user) = result {
            fetchedUser = user
        }
        expectation.fulfill()
    }
    
    // Assert
    wait(for: [expectation], timeout: 1.0)
    XCTAssertEqual(fetchedUser?.id, "123")
    XCTAssertEqual(fetchedUser?.name, "Test User")
    XCTAssertEqual(fetchedUser?.email, "test@example.com")
}
```

### Dependency Injection

- Design classes to accept dependencies via **initializer injection** (preferred) or property injection. This is crucial for testability and follows SOLID principles.
- Use protocol types rather than concrete implementations for dependencies.

```swift
// Using the mock in tests
func test_fetchUserProfile_withSuccess_returnsUserObject() {
    // Arrange
    let mockService = MockNetworkService()
    let userJSON = """
    {"id": "123", "name": "Test User", "email": "test@example.com"}
    """.data(using: .utf8)!
    
    mockService.dataToReturn = userJSON
    let userRepository = UserRepository(networkService: mockService)
    
    let expectation = XCTestExpectation(description: "Fetch user profile")
    var fetchedUser: User?
    
    // Act
    userRepository.fetchUser(id: "123") { result in
        if case .success(let user) = result {
            fetchedUser = user
        }
        expectation.fulfill()
    }
    
    // Assert
    wait(for: [expectation], timeout: 1.0)
    XCTAssertEqual(fetchedUser?.id, "123")
    XCTAssertEqual(fetchedUser?.name, "Test User")
    XCTAssertEqual(fetchedUser?.email, "test@example.com")
}
```

## Test Organization and Structure

### File Organization

- Group test files mirroring the app\'s structure (e.g., by feature).
- Keep test files focused (SRP).

```
YourApp/
└── Tests/
    ├── UnitTests/
    │   ├── Models/
    │   ├── ViewModels/
    │   └── Services/
    ├── IntegrationTests/
    │   ├── Networking/
    │   └── Persistence/
    ├── UITests/
    │   ├── Flows/
    │   └── Screens/
    └── TestUtilities/
        ├── Mocks/
        ├── Factories/
        └── Extensions/
```

### Continuous Integration

- Configure test schemes for different test types
- Set up CI to run tests on each PR and after merges
- Track test coverage (using tools like Slather or Codecov)
- Make test failures visible and actionable for the team

## Testing Tips for SwiftUI

- Test business logic primarily in **ViewModels** or other logic containers, keeping Views simple.
- Use `ViewInspector` or similar tools cautiously for view structure testing if absolutely necessary.
- Focus on testing state changes and their effects, aligning with the declarative nature of SwiftUI.

```swift
// Testing a SwiftUI ViewModel
func test_toggleFavorite_updatesIsFavoriteProperty() {
    // Arrange
    let viewModel = ProductViewModel(product: testProduct)
    
    // Act
    viewModel.toggleFavorite()
    
    // Assert
    XCTAssertTrue(viewModel.isFavorite)
    
    // Act again
    viewModel.toggleFavorite()
    
    // Assert again
    XCTAssertFalse(viewModel.isFavorite)
}
```

## Testing Legacy Code

- Focus on high-value areas first. Add tests before refactoring.
- Use the **Humble Object pattern** to separate testable logic from difficult-to-test UI or framework code.
- Apply **Characterization Testing** (writing tests to document existing behavior) before refactoring undocumented code.

Remember that good tests are investments in your codebase\'s long-term health and resilience. They provide confidence in refactoring, document expected behavior, enforce SOLID principles, and catch regressions early. Write tests that are useful, maintainable, reliable, and drive the development process (TDD).
